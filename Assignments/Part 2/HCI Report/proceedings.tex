\documentclass{sigchi}

% Use this section to set the ACM copyright statement (e.g. for
% preprints).  Consult the conference website for the camera-ready
% copyright statement.

% Copyright

%\setcopyright{acmcopyright}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}
% DOI
\doi{https://doi.org/10.1145/3313831.XXXXXXX}
% ISBN
\isbn{978-1-4503-6708-0/20/04}
%Conference
\conferenceinfo{CHI'20,}{April  25--30, 2020, Honolulu, HI, USA}
%Price
\acmPrice{\$15.00}

% Use this command to override the default ACM copyright statement
% (e.g. for preprints).  Consult the conference website for the
% camera-ready copyright statement.

%% HOW TO OVERRIDE THE DEFAULT COPYRIGHT STRIP --
%% Please note you need to make sure the copy for your specific
%% license is used here!
% \toappear{
% Permission to make digital or hard copies of all or part of this work
% for personal or classroom use is granted without fee provided that
% copies are not made or distributed for profit or commercial advantage
% and that copies bear this notice and the full citation on the first
% page. Copyrights for components of this work owned by others than ACM
% must be honored. Abstracting with credit is permitted. To copy
% otherwise, or republish, to post on servers or to redistribute to
% lists, requires prior specific permission and/or a fee. Request
% permissions from \href{mailto:Permissions@acm.org}{Permissions@acm.org}. \\
% \emph{CHI '16},  May 07--12, 2016, San Jose, CA, USA \\
% ACM xxx-x-xxxx-xxxx-x/xx/xx\ldots \$15.00 \\
% DOI: \url{http://dx.doi.org/xx.xxxx/xxxxxxx.xxxxxxx}
% }
\toappear{}

% Arabic page numbers for submission.  Remove this line to eliminate
% page numbers for the camera ready copy
% \pagenumbering{arabic}

% Load basic packages
\usepackage{balance}       % to better equalize the last page
\usepackage{graphics}      % for EPS, load graphicx instead 
\usepackage[T1]{fontenc}   % for umlauts and other diaeresis
\usepackage{txfonts}
\usepackage{mathptmx}
\usepackage[pdflang={en-US},pdftex]{hyperref}
\usepackage{color}
\usepackage{booktabs}
\usepackage{textcomp}

% Some optional stuff you might like/need.
\usepackage{microtype}        % Improved Tracking and Kerning
% \usepackage[all]{hypcap}    % Fixes bug in hyperref caption linking
\usepackage{ccicons}          % Cite your images correctly!
% \usepackage[utf8]{inputenc} % for a UTF8 editor only

% If you want to use todo notes, marginpars etc. during creation of
% your draft document, you have to enable the "chi_draft" option for
% the document class. To do this, change the very first line to:
% "\documentclass[chi_draft]{sigchi}". You can then place todo notes
% by using the "\todo{...}"  command. Make sure to disable the draft
% option again before submitting your final document.
\usepackage{todonotes}

% Paper metadata (use plain text, for PDF inclusion and later
% re-using, if desired).  Use \emtpyauthor when submitting for review
% so you remain anonymous.
\def\plaintitle{Preventing Bias in Machine Learning by using Bias Aware Algorithms: An Empirical Experimental Study - Full Report}
\def\plainauthor{First Author, Second Author, Third Author,
  Fourth Author, Fifth Author, Sixth Author}
\def\emptyauthor{}
\def\plainkeywords{Authors' choice; of terms; separated; by
  semicolons; include commas, within terms only; this section is required.}
\def\plaingeneralterms{Documentation, Standardization}

% llt: Define a global style for URLs, rather that the default one
\makeatletter
\def\url@leostyle{%
  \@ifundefined{selectfont}{
    \def\UrlFont{\sf}
  }{
    \def\UrlFont{\small\bf\ttfamily}
  }}
\makeatother
\urlstyle{leo}

% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{12in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}

% Make sure hyperref comes last of your loaded packages, to give it a
% fighting chance of not being over-written, since its job is to
% redefine many LaTeX commands.
\definecolor{linkColor}{RGB}{6,125,233}
\hypersetup{%
  pdftitle={\plaintitle},
% Use \plainauthor for final version.
%  pdfauthor={\plainauthor},
  pdfauthor={\emptyauthor},
  pdfkeywords={\plainkeywords},
  pdfdisplaydoctitle=true, % For Accessibility
  bookmarksnumbered,
  pdfstartview={FitH},
  colorlinks,
  citecolor=black,
  filecolor=black,
  linkcolor=black,
  urlcolor=linkColor,
  breaklinks=true,
  hypertexnames=false
}

% create a shortcut to typeset table headings
% \newcommand\tabhead[1]{\small\textbf{#1}}

% End of preamble. Here it comes the document.
\begin{document}

\title{\plaintitle}

\numberofauthors{3}
\author{%
  \alignauthor{Andy Gray\\
    \affaddr{445348}\\
    \email{445348@swansea.ac.uk}}
}



\maketitle

%\begin{abstract}
%	content...
%\end{abstract}


\section{Introduction}

%[Project Discription]\\
	We aim to remove the potential for gender bias in a suggested pay to an employee from data with a clear gender bias within the dataset. Through using pre and post-processing data methods to identify the bias and then aim to remove it. Due to the years of women and men getting paid different amounts. Even though laws have come into power to prevent such a thing from happening, but yet still does.

%[Motivation]\\
	In 2018, women, no matter their background, on average earned just 82 cents for every \$1 earned by men \cite{1}. Machine Learning (ML) requires vast amounts of past data to inform future events, with AI and ML being the key driver behind many decisions. However, with there being a well-known gap between a person's gender and their pay, the ML models will only learn this and use this as a factor in their decisions making. Therefore, to stop this from happening, a system needs to be put into place to remove this process's bias.

	Using fairness techniques at pre and post-processing stages \cite{three} of supervised learning, we will aim to remove the bias of someone's gender from a suggested pay salary for an individual. %Needs changing -> copy job from part 1.

%[Summary of existing lit]\\

%blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah

%[Problems with lit]\\
	Mothers in part-time jobs are getting hit by a "pay penalty" and are often not given pay rises linked to experience \cite{bbc_mothers_suffering}. Additionally, "more than three out of four UK companies pay their male staff more than their female staff, and in nine out of 17 sectors in the economy, men earn 10\% or more on average than women" \cite{gender_pay_FT}.% - due to pay scales

	Therefore, even though by law in the UK, it is illegal to pay men and women different amounts for doing the same job, a gender pay gap still exists, and there is still a difference in equal pay. This difference in equal pay is driven by the use of salary pay scale ranges and, in some cases, a bias to think that women are not as valuable as men within a similar role.
	
	So we are proposing a solution that will help reduce the bias within salary negotiations that removes the elements of gender bias to produce a tool that will look at the person's job role and their years of experience. Additionally, not taking into account previous pay and if they were working full or part-time.

%[Project Spec]\\ % Maybe not needed.
%blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah
	Therefore, we will create a model that will predict a recommended employees wage based on their years of experience, not taking into account their gender or if they had previously worked part-time or not. The differences will be found in the two genders wages to neutralise these differences so that the model will be almost identical for both men and women. Doing this should also take away any potential unconscious bias that the employer might have regarding what amount of pay to give to the employee.
	
	This model will get achieved by using mitigating bias methods in pre and post-processing. By finding out where the bias is, we can then change decision boundaries to present a fairer outcome based on only years of experience.

%[Result findings]\\
%blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah blah
	The model was able to predict both men and women on an almost identical linear prediction. Only very slight differences were between the different genders and their linear plotted regression lines. These differences could get removed entirely with additional minor tweaks to the model parameters. Overall, the result was very positive.

%[Overview]\\
	We will next look into the background of this topic and review existing literature. Then an explain of the study design, the libraries used to create the solution, the dataset used, and the pre and post data processing techniques used. We will then present the results and analyse them. Finally, a discussion on the empirical study and concluding the overall findings.

\section{Background \& Literature Review}
	%Gender Pay Gap
	In 2018 companies with 250 or more employees were expected to file their first data on their gender pay gap data. With the actions of companies publishing their data, this created a significant discussion around what people earn and ultimately what difference in what men and women earn \cite{bbc_gender_pay_gap}. In 2017 a UK male earned 18.4\% more than a woman \cite{ons_gender_pay_gap_17}. 
	
	It is important to note that the pay gap is not the same thing as equal pay. Equal pay is a law that got legislated in 1970 that ensures that men and women doing the same job should get paid the same amount \cite{bbc_gender_pay_gap}. Due to the equal pay act 1970 \cite{equal_pay_act_1070} and the equality act 2010 \cite{equal_act_2010}, it is illegal to pay works different amounts for the same man if they are a man or woman.
	
	While companies like DDB UK, the legal entity, which includes Adam \& EveDDB, Tribal Worldwide, Gutenberg Global and Cain \& Abel, releases detailed gender pay gap reports each year. The company has, since 2017, enforced a rule that 50\% of the candidates for senior positions getting interviewed must be female. Additionally, they are initiating mandatory unconscious bias training \cite{uk_gender_gap}. However, while making these changes, a recent slowdown in reducing the gap has occurred. Additionally, the gender pay gap between graduates has not improved since 1993. There have been no improvements despite the gap getting a reduction for non-graduates \cite{bbc_mothers_suffering}.
	
	Mothers in part-time jobs are getting hit by a "pay penalty" and are often not given pay rises linked to experience \cite{bbc_mothers_suffering}.
	By the time a couple's first child is aged 20, many mothers earn nearly a third less than the fathers. A key factor was women working part-time in motherhood \cite{ifs_mothers_suffer}. This situation that mothers find themselves in gets referred to as the "motherhood penalty" \cite{tuc_motherhood}. For mothers born in 1970, there is a 34\% overall gender pay gap. This gap is primarily due to the impact of parenthood on earnings, with women earning less and men earning more after having children. However, there is a significant but much smaller gender pay gap between childless women and men born at this time, a 12\% gap \cite{tuc_motherhood}.
	
	
	In 2020, "More than three out of four UK companies pay their male staff more than their female staff, and in nine out of 17 sectors in the economy, men earn 10\% or more on average than women" \cite{gender_pay_FT}. This is an issue generated when workers get paid, on a scale, based on their "experience". It opens up an opportunity for employees to be paid different amounts based on what is perceived to be their worth, which can have an unconscious bias at the heart of it. % [As when people need to negotiate for their salaries men will think they are worth more than they actually are, while women will usually take the first offer]. 
	
	As we have already mentioned, bias exists whether it be conscious or unconsciously within everyday life. However, these biases that we humans have will only make those biases even more prominent when used to train ML models. This bias could be visibly evident or could go without a trace, but the models will only learn from past data to predict the future, and if that passed data is biased, the future will be significantly biased.
	
	There are several ways that a model can become biased. These include: sampling bias, measurement bias, exclusion bias, experimenter or observer bias, prejudicial bias, conformational bias, and bandwagoning or bandwagon effect \cite{6_ways_bias}. 
	
	We need to make sure that bias does not become ingrained within our ML models. When it does, it can harm our daily lives. The bias can get manifested in exclusion, such as certain groups getting denied loans or technology not working the same for everyone. As AI becomes more a part of our lives, the dangers from bias only grow larger \cite{6_ways_bias}.
	
	There are multiple ways to reduce bias (see fig: \ref{fig:bias_advice}). The first thing that is required is to understand what the bias is \cite{three}. In order to understand the bias, this stage has three sub-sections within it, Socio-technical causes of bias, bias manifestation in data, fairness definition \cite{three}. Another way is by aiming to mitigate the bias \cite{three}. This stage involves preprocessing, in-processing and post-processing the data and model \cite{three}. The final method is accounting for bias \cite{three}. This method involves there being a bias-aware data collection, describing and modelling bias and finally explaining ai decisions \cite{three}. 
	
	\begin{figure}[h]
		\includegraphics[width=9.2cm]{bias_advice.png}
				\caption{Overview of the areas of bias to consider \cite{three}.} %\cite{three}
		\label{fig:bias_advice}
		\centering
	\end{figure}
	
	

\subsection{Study Design}
	Our study is around the topic of bias in algorithms and looking at ways to remove these biases. The study will be looking at ways to detect the bias, measure it and then reduce it. Our study got carried out in a manner that follows an empirical experimental study method. 

	Our study has looked into ways to identify bias within a dataset and then look at ways to remove this bias, ensuring that protected characteristics, in our case gender, do not impact or impede a person's proposed suggested salary.  

	We aimed to try and find out if there was first any bias within the dataset. We did this by first plotting out the dataset based on the characteristic of male and female. We initially created a model that would truthfully represent the gender bias within the model's predictions.

	To achieve removing the bias, we extracted the prediction-specific interactions. By getting the interactions, we could cancel out their effect and the influence driven by the gender variable. We then re-calculate our predictions, which immediately shows the removal of any sign of prejudice within the dataset. Additionally, the model conscientiously captures the variation driven by the employee's years of employment and career path. 

	This bias-aware approach to modelling can be applied to other forms of input types, with a similar approach being used by Google \cite{google_ref}

\subsection{Libraries}
	We used Python 3 \cite{python} to create the empirical experiment. Additional libraries used were Pandas \cite{pandas} to allow us to load in the data and wrangle the data frames. Seaborn \cite{seaborn} was also used to visualise the data. XGBoost \cite{xgboost} to create the model and extract the critical interactions from within the model. 

\subsection{Dataset and Data Pre \& Post-processing}
	We create our dataset to simulate data containing gender, years of experience, and career type. Our overall aim is to predict the salary of someone while taking these features into account. The type of career will be focusing on software engineering (SWE) and consulting.  We decided to create this dataset synthetically due to time restrictions and lack of data collection containing real-world figures of these two career options. Most datasets found provided more of an overview than the exact figures, and we believe this is due to the sensitive and personal nature of the required data.
	
	We based the synthesised data on general assumptions about pay. There will be a clear positive relationship between years of experience and a person's salary. An SWE will earn less than a consultant, and being male will earn them more money than females generally. Additional factors within the data are that in SWE roles, women will start at the lower end of the scale while men will be varied and, therefore, women will be over time increasing their pay. However, this increase will be at a faster rate than men but from a lower starting point. While for consulting, both males and females will start at the same rate, but men will get more considerable increases in pay over time compared to their women counterparts. 
	
	%[data preprocessing]\\
	As some of the columns contained text categorical data types, we first processed the data to convert this to a numerical value. The data got split into a train test set of 70/30\%. 
	
	%[data postprocessing]\\
	We ensured that when the model was making its predictions, the predictions' interactions got provided with the results. We then created a bias variable based on the model's gender values and a bias index based on the values being in the datasets feature names and the bias-variance. 
	
	We were then able to create a de-biased $\hat{y}$ value by summing the interaction values and then minus 1. The outputted value then has created the new values with the gender bias removed.


	%Synthetic dataset based around two job types -> why because was hard to find data set that does not provide "just an overview". 

	%pandas.get\_dummies

\subsection{Parameter Settings}
	We set the learning rate to the model as 0.1, between the range of 0 and 1 that the model expects. The learning task for the model got set to regression squared error, which looks at the regression squared loss. The number of boost rounds got set to 100 as well.

\section{Results \& Analysis}
	
	When we look at the dataset in its original representation (see fig: \ref{fig:original_data}), we can see a clear divide between the male and female pay salaries. We can see that the regression line for SWE is \textasciitilde{42,000} for women and \textasciitilde{60,000} for men starting, and then both these gender values increase to \textasciitilde{105,000}. Therefore both values roughly reaching the same value at 20 years of experience. While for consulting, the starting off values are very close to each other, \textasciitilde{60-65,000}. However, these values completely deviate when at the top end of the scale. Men have a value of \textasciitilde{115,000}, while women have a value of \textasciitilde{105,000}.

	\begin{figure}[h]
		\includegraphics[width=9.2cm]{original_data.png}
		\caption{The dataset's datapoints and initial regression fit with the data being unmodified, split by gender and career option.}
		\label{fig:original_data}
		\centering
	\end{figure}

	Before removing the gender bias, we can see that the model's predictions mirror a similar result to the original data's general trend for SWE (see fig: \ref{fig:se_comparison}).  However, when we look at the values when the gender bias gets removed, the model's predictions almost align perfectly with each other. It is evident that there is some difference between the two gender predictions, but the difference is extremely marginal when concerning SWE employees. With both men and women being \textasciitilde{65,000} starting and \textasciitilde{112,000} at 20 years of experience.
	
	\begin{figure}[h]
		\includegraphics[width=9.2cm]{se_comparison.png}
		\caption{The model's 'Software Engineering' initial $\hat{y}$ regression prediction with bias and the $\hat{y}$ predition after the bias has been removed.}
		\label{fig:se_comparison}
		\centering
	\end{figure}

	In relations to consulting, we can see that the predictions with the bias still intact, the model's predictions follow the same patterns as the data overall (see fig: \ref{fig:consulting_comparison}).  The regression representation is close to being equally matched. With the starting off wage for both men and women around \textasciitilde{60-65,000} and the top end of the range being \textasciitilde{109-111,000}. It is evident that there is some difference between the two gender predictions, but the difference is very marginal concerning consulting employees. 

	\begin{figure}[h]
		\includegraphics[width=9.2cm]{consulting_comparison.png}
		\caption{The model's 'Consulting' initial $\hat{y}$ regression prediction with bias and the $\hat{y}$ predition after the bias has been removed.}
		\label{fig:consulting_comparison}
		\centering
	\end{figure}

	Therefore, overall we can see that the approach has created almost perfect un-bias results between genders over the different career options. While it has not matched the values up entirely between the genders, it has created a much more even representation than prior. 

	% Maybe add in RMSPE and other metrics in as well possibly?

\section{Discussion \& Conclusion}
	%[discussion]\\
	Overall, we can see that the gender aspect now has little impact on the model's overall predictions. However, while the bias has not entirely removed, it has dramatically reduced and almost disappeared. Therefore we believe that further investigation is required to eradicate this, but we can see we are taking steps in the right direction.  
	
	This model shows is that when using salaries based on the years of experience, when can remove the gender bias within the data. however, further exploration is required to see if this is still the same case when looking at other professions, to see if there might be a slight difference in salaries but maybe not a significant difference, to whether this technique will still have the same positive outcome. We believe it will still be able to be used across the board on similar data sets.
	
	%[conclusion] \\
	While we can say, more work needs to get done to determine the approaches robustness in other contexts. It is clear that this approach will harm the model's accuracy metric scores. Especially when we compare the results, our de-biased model predicts compared to our testing data's actual results. We believe this decrease in the model's performance gets justified due to the model's ability to create a more levelled playing field between men and women. Using their years of experience as the main driving force and nothing else when predicting an employee's potential pay amount. 
	
	As women get penalised for working part-time and their years of experience not taken into account, this has lead to women getting paid a lot less than men. Therefore, we believe this method would stop any individual, whether it be men or women, who decide to go part-time at any point in their lives get treated with the same opportunity in potential pay. Especially compared to full-time workers, as they might be working full time, the experience is still the same. However, ultimately the model will aim to remove years of deep bias in our methods and history in pay amounts awarded to men and women. Therefore, by building a model that does not prejudice through its training data, we must first let the model first measure the amount of prejudice. We can then reset the bias contributing factors to zero.
	\newpage

\newpage
% BALANCE COLUMNS
\balance{}
% REFERENCES FORMAT
% References must be the same font size as other body text.
\bibliographystyle{SIGCHI-Reference-Format}
\bibliography{sample}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
